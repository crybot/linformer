model:
  type: Vanilla transformer
  dim: 512
  mlp_dim: 2048
  n_heads: 8
  n_layers: 6
training:
  epochs: 10
  batch_size: 88
  learning_rate: 1.0e-4
  lr_scheduler:
    warmup_steps: 4000
    cosine_annealing: True
    cosine_tmax: 10
    cosine_factor: 1.0
    cosine_restart: False
    min_lr: 1.0e-6
dataset:
  max_length: 256 # TODO: get from dataset if pretokenized
