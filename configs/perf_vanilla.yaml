model:
  type: Vanilla transformer
  dim: 512
  mlp_dim: 2048
  n_heads: 8
  n_layers: 6
  vocab_size: 50272
  encoder_only: false
training:
  batch_size: 256
dataset:
  max_length: 256
