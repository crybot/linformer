#TODO: batch_size is not used, a max_tokens field would be useful instead
model:
  type: Vanilla transformer
  dim: 512
  mlp_dim: 2048
  n_heads: 8
  n_layers: 6
  vocab_size: 50272
  encoder_only: true
training:
  batch_size: 256
dataset:
  max_length: 256
